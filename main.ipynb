{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f16163d2dd773fbc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Binary Classification \n",
    "### Introduction\n",
    "In this project, we make decisions on food taste similarity based on image and human judgements. We are provided with a dataset of images of 10'000 dishes. \n",
    "<br>\n",
    "<img src=\"image.png\" alt=\"Food Dish\" width=\"600\" height=\"500\"> <br>\n",
    "<br>\n",
    "Furthermore, we are provided with a set of triples $(A, B, C)$ representing human annotations, meaning that dish $A$ is more similar to the taste of dish $B$ than to the taste of dish $C$. \n",
    "\n",
    "Our task is to predict for unseen tripbles $(A, B, C)$ whether dish $A$ is more similar in taste to $B$ or $C$. More precisely, for each triple in `test_triplets.txt` predict $0$ or $1$ as follows: \n",
    "- $1$ if the dish in image `A.png` is closter in taste to the dish in image `B.png` than to the dish in `C.png`\n",
    "- $0$ if the dish in image `A.png` is closer in taste to the dish in image `C.png` than to the dish in `B.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1a3a9db8e3f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Imports \n",
    "First, we import necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824a840beb8b323e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82adb41ca8c23be6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a25c7-3889-4c46-8e77-1ed76c60f380",
   "metadata": {},
   "source": [
    "### Extracting the Embeddings using pretrained Model\n",
    "Fist, we have to transform, resize and normalize the images. Then we use a pretrained model to extract the embeddings. \n",
    "References: https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n",
    "\n",
    "Comments: \n",
    "- since we train our model on GPUs, set `pin_memory=True` to enable faster data transfer between CPU and GPU\n",
    "- we have 128 cores, thus `num_workers=128`\n",
    "- embeddings $=$ compact, numerical representation of images. The purpose of an embedding is to capture the most important features of the image. Embeddings typically represent things like edges, textures, colors.\n",
    "- extracting embeddings $=$ feed images through pretrained model and collect the output from intermediate layer (right before the final classification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3d5c760c9c963b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "training_data = datasets.ImageFolder(\n",
    "    root=\"dataset/\", \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    pin_memory=True, \n",
    "    num_workers=64, \n",
    ")\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()\n",
    "\n",
    "batch_size = 64\n",
    "embedding_size = 2048 \n",
    "num_images = len(training_data)\n",
    "embeddings = np.zeros((num_images, embedding_size))\n",
    "\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    for (X, y) in train_dataloader:\n",
    "        X = X.to(device)\n",
    "        batch_embeddings = model(X)\n",
    "        batch_embeddings = batch_embeddings.view(batch_embeddings.size(0), -1).cpu().numpy() # vectorize to numpy array \n",
    "        batch_size = batch_embeddings.shape[0]\n",
    "        embeddings[idx : idx + batch_size] = batch_embeddings\n",
    "        idx += batch_size\n",
    "\n",
    "np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae8ed5-c68b-4ff8-9dbb-5c56b84a7b93",
   "metadata": {},
   "source": [
    "### Use Embeddings as Training Data\n",
    "After generating the embeddings for the images, we now need to create the appropriate training data. More precisely, for a random sample, e.g., `02461 03450 02678` from `train_triplets.txt`, we need to find the corresponding embeddings and create the correct label. We use a mapping to associate filenames with embeddings. In this case, we have the label 1, and we apply data augmentation to add `02461 02678 03450` with label 0. In summary, the function `get_data` transforms the training data (and test data) into matrices $X$ and $y$, storing the corresponding embeddings and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335d91cc379d4f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def l2_normalize(embeddings):\n",
    "    norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norm \n",
    "\n",
    "# X contains embeddings for training data \n",
    "# y contains normal labels, i.e. 0 or 1 \n",
    "def get_data(file, train=True):\n",
    "    # read triplets into array \n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from triplets\n",
    "    training_data = datasets.ImageFolder(root=\"dataset/\", transform=None)\n",
    "    # contains 00000, 00001, ...\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in training_data.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    embeddings = l2_normalize(embeddings)\n",
    "\n",
    "    # maps filenames to their corresponding embedding, e.g. 00000 -> [x,y,z,...]\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1) # since dish is closer to B then to C \n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)  \n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48f07a1c0c478",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The function `create_loader_from_np` creates an instance of the iterable class `DataLoader` for our training- and testset . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6daf836a4adb0abe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y = None, train = True, batch_size=64, shuffle=True, num_workers = 4):\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1b0092e0b13f88",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "train_dataloader = create_loader_from_np(X, y, train = True, batch_size=64)\n",
    "del X\n",
    "del y\n",
    "\n",
    "X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n",
    "test_dataloader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6df5b9-6096-4524-b72e-3265043efb83",
   "metadata": {},
   "source": [
    "### Building my Neural Network \n",
    "We can use a simple neural network for classification since we have already extracted the necessary information about the images and stored it in `embeddings.npy`. My approach includes a neural network with two hidden layers, ReLU and Sigmoid activation functions, and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd11318eb7b9488",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CovNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(6144, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "           \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28634c90281cd699",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device).float()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            loss = loss.item()\n",
    "            current = batch_idx * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ea99b26c348253",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn=None):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for [X] in dataloader:  # assuming dataloader provides only inputs\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            pred = torch.round(pred)  # if you want binary predictions\n",
    "            pred = pred.cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)  # ensure consistent shape\n",
    "    np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "    print(\"Results saved to results.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ecad3-4ab3-4542-9b11-bc342c402da7",
   "metadata": {},
   "source": [
    "### Training & Making Predictions using Neural Network\n",
    "After defining the neural network, we can finally choose our hyperparameters, such as learning rate, number of epochs, and batch size. I selected Binary Cross Entropy loss as the loss function and the Adam optimization algorithm.\n",
    "\n",
    "We can see that the model is trained properly since the training loss decreases to less than 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb07ad51-bc3e-470f-8603-ad4e194e1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.710329  [   64/119030]\n",
      "loss: 0.716275  [32064/119030]\n",
      "loss: 0.678448  [64064/119030]\n",
      "loss: 0.686482  [96064/119030]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.637854  [   64/119030]\n",
      "loss: 0.673233  [32064/119030]\n",
      "loss: 0.652536  [64064/119030]\n",
      "loss: 0.620008  [96064/119030]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.612144  [   64/119030]\n",
      "loss: 0.573645  [32064/119030]\n",
      "loss: 0.548633  [64064/119030]\n",
      "loss: 0.606827  [96064/119030]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.628059  [   64/119030]\n",
      "loss: 0.575090  [32064/119030]\n",
      "loss: 0.541187  [64064/119030]\n",
      "loss: 0.430381  [96064/119030]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.400872  [   64/119030]\n",
      "loss: 0.440567  [32064/119030]\n",
      "loss: 0.406173  [64064/119030]\n",
      "loss: 0.404736  [96064/119030]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.360248  [   64/119030]\n",
      "loss: 0.298176  [32064/119030]\n",
      "loss: 0.407631  [64064/119030]\n",
      "loss: 0.244681  [96064/119030]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.310364  [   64/119030]\n",
      "loss: 0.365608  [32064/119030]\n",
      "loss: 0.331770  [64064/119030]\n",
      "loss: 0.398825  [96064/119030]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.268111  [   64/119030]\n",
      "loss: 0.189032  [32064/119030]\n",
      "loss: 0.181991  [64064/119030]\n",
      "loss: 0.325858  [96064/119030]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.184651  [   64/119030]\n",
      "loss: 0.119790  [32064/119030]\n",
      "loss: 0.283671  [64064/119030]\n",
      "loss: 0.201618  [96064/119030]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.107635  [   64/119030]\n",
      "loss: 0.106687  [32064/119030]\n",
      "loss: 0.163708  [64064/119030]\n",
      "loss: 0.193765  [96064/119030]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.114819  [   64/119030]\n",
      "loss: 0.172919  [32064/119030]\n",
      "loss: 0.221198  [64064/119030]\n",
      "loss: 0.172634  [96064/119030]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.115410  [   64/119030]\n",
      "loss: 0.170499  [32064/119030]\n",
      "loss: 0.172204  [64064/119030]\n",
      "loss: 0.173869  [96064/119030]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.072902  [   64/119030]\n",
      "loss: 0.069284  [32064/119030]\n",
      "loss: 0.051859  [64064/119030]\n",
      "loss: 0.072127  [96064/119030]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.050582  [   64/119030]\n",
      "loss: 0.138552  [32064/119030]\n",
      "loss: 0.093663  [64064/119030]\n",
      "loss: 0.090077  [96064/119030]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.106662  [   64/119030]\n",
      "loss: 0.051796  [32064/119030]\n",
      "loss: 0.155656  [64064/119030]\n",
      "loss: 0.114451  [96064/119030]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.088664  [   64/119030]\n",
      "loss: 0.176058  [32064/119030]\n",
      "loss: 0.230147  [64064/119030]\n",
      "loss: 0.050263  [96064/119030]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.086483  [   64/119030]\n",
      "loss: 0.051462  [32064/119030]\n",
      "loss: 0.047853  [64064/119030]\n",
      "loss: 0.071975  [96064/119030]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.064634  [   64/119030]\n",
      "loss: 0.056898  [32064/119030]\n",
      "loss: 0.043153  [64064/119030]\n",
      "loss: 0.034405  [96064/119030]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.097065  [   64/119030]\n",
      "loss: 0.030018  [32064/119030]\n",
      "loss: 0.045013  [64064/119030]\n",
      "loss: 0.084668  [96064/119030]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.041460  [   64/119030]\n",
      "loss: 0.033155  [32064/119030]\n",
      "loss: 0.028444  [64064/119030]\n",
      "loss: 0.048530  [96064/119030]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.062434  [   64/119030]\n",
      "loss: 0.046872  [32064/119030]\n",
      "loss: 0.057626  [64064/119030]\n",
      "loss: 0.040815  [96064/119030]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.081231  [   64/119030]\n",
      "loss: 0.032949  [32064/119030]\n",
      "loss: 0.077892  [64064/119030]\n",
      "loss: 0.057551  [96064/119030]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.059873  [   64/119030]\n",
      "loss: 0.040198  [32064/119030]\n",
      "loss: 0.082626  [64064/119030]\n",
      "loss: 0.044735  [96064/119030]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.049181  [   64/119030]\n",
      "loss: 0.064475  [32064/119030]\n",
      "loss: 0.034054  [64064/119030]\n",
      "loss: 0.159539  [96064/119030]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.082504  [   64/119030]\n",
      "loss: 0.022902  [32064/119030]\n",
      "loss: 0.133260  [64064/119030]\n",
      "loss: 0.012485  [96064/119030]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.012125  [   64/119030]\n",
      "loss: 0.019126  [32064/119030]\n",
      "loss: 0.042616  [64064/119030]\n",
      "loss: 0.040112  [96064/119030]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.048527  [   64/119030]\n",
      "loss: 0.024049  [32064/119030]\n",
      "loss: 0.073693  [64064/119030]\n",
      "loss: 0.039408  [96064/119030]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.016834  [   64/119030]\n",
      "loss: 0.022002  [32064/119030]\n",
      "loss: 0.021301  [64064/119030]\n",
      "loss: 0.062841  [96064/119030]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.008949  [   64/119030]\n",
      "loss: 0.010628  [32064/119030]\n",
      "loss: 0.131504  [64064/119030]\n",
      "loss: 0.013373  [96064/119030]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.059741  [   64/119030]\n",
      "loss: 0.039956  [32064/119030]\n",
      "loss: 0.059012  [64064/119030]\n",
      "loss: 0.014057  [96064/119030]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.016449  [   64/119030]\n",
      "loss: 0.089588  [32064/119030]\n",
      "loss: 0.081032  [64064/119030]\n",
      "loss: 0.035048  [96064/119030]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.046899  [   64/119030]\n",
      "loss: 0.029654  [32064/119030]\n",
      "loss: 0.058784  [64064/119030]\n",
      "loss: 0.007557  [96064/119030]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.013925  [   64/119030]\n",
      "loss: 0.056220  [32064/119030]\n",
      "loss: 0.025540  [64064/119030]\n",
      "loss: 0.019926  [96064/119030]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.032905  [   64/119030]\n",
      "loss: 0.012915  [32064/119030]\n",
      "loss: 0.052310  [64064/119030]\n",
      "loss: 0.028454  [96064/119030]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.019087  [   64/119030]\n",
      "loss: 0.046463  [32064/119030]\n",
      "loss: 0.012104  [64064/119030]\n",
      "loss: 0.018374  [96064/119030]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.012800  [   64/119030]\n",
      "loss: 0.022465  [32064/119030]\n",
      "loss: 0.024225  [64064/119030]\n",
      "loss: 0.010435  [96064/119030]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.064890  [   64/119030]\n",
      "loss: 0.009019  [32064/119030]\n",
      "loss: 0.029547  [64064/119030]\n",
      "loss: 0.010493  [96064/119030]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.050999  [   64/119030]\n",
      "loss: 0.005934  [32064/119030]\n",
      "loss: 0.083919  [64064/119030]\n",
      "loss: 0.032486  [96064/119030]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.025706  [   64/119030]\n",
      "loss: 0.009538  [32064/119030]\n",
      "loss: 0.003972  [64064/119030]\n",
      "loss: 0.045071  [96064/119030]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.049083  [   64/119030]\n",
      "loss: 0.030171  [32064/119030]\n",
      "loss: 0.048248  [64064/119030]\n",
      "loss: 0.043494  [96064/119030]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.021365  [   64/119030]\n",
      "loss: 0.002605  [32064/119030]\n",
      "loss: 0.011487  [64064/119030]\n",
      "loss: 0.039896  [96064/119030]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.011311  [   64/119030]\n",
      "loss: 0.006560  [32064/119030]\n",
      "loss: 0.063575  [64064/119030]\n",
      "loss: 0.083536  [96064/119030]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.083689  [   64/119030]\n",
      "loss: 0.015143  [32064/119030]\n",
      "loss: 0.017174  [64064/119030]\n",
      "loss: 0.024905  [96064/119030]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.046912  [   64/119030]\n",
      "loss: 0.009480  [32064/119030]\n",
      "loss: 0.010990  [64064/119030]\n",
      "loss: 0.041698  [96064/119030]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.027853  [   64/119030]\n",
      "loss: 0.006776  [32064/119030]\n",
      "loss: 0.006931  [64064/119030]\n",
      "loss: 0.021066  [96064/119030]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.005150  [   64/119030]\n",
      "loss: 0.048328  [32064/119030]\n",
      "loss: 0.069505  [64064/119030]\n",
      "loss: 0.004733  [96064/119030]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.014436  [   64/119030]\n",
      "loss: 0.021276  [32064/119030]\n",
      "loss: 0.041615  [64064/119030]\n",
      "loss: 0.013119  [96064/119030]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.009449  [   64/119030]\n",
      "loss: 0.013253  [32064/119030]\n",
      "loss: 0.077914  [64064/119030]\n",
      "loss: 0.012834  [96064/119030]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.013000  [   64/119030]\n",
      "loss: 0.018454  [32064/119030]\n",
      "loss: 0.015253  [64064/119030]\n",
      "loss: 0.136981  [96064/119030]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.043491  [   64/119030]\n",
      "loss: 0.023585  [32064/119030]\n",
      "loss: 0.016922  [64064/119030]\n",
      "loss: 0.023210  [96064/119030]\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "model = CovNet()\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 1E-3\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a0a3d-de4b-47c6-8979-f0823cb0f947",
   "metadata": {},
   "source": [
    "After training is compmleted, we can now make our predictions for the test data and save it in `results.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060e40d2-4a61-4e4f-9587-b229b8ddd18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
